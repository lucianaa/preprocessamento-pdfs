{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "548f65e6-912a-465d-ab8f-b7e00b749d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-text-splitters in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from langchain-text-splitters) (1.0.3)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.4.41)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (23.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.12.4)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (6.0.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (8.2.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.4.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.3.0)\n",
      "Requirement already satisfied: langchain-huggingface in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from langchain-huggingface) (0.36.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.3 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from langchain-huggingface) (1.0.3)\n",
      "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from langchain-huggingface) (0.22.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.15.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (0.4.41)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (2.12.4)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (8.2.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (2.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.10.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (0.4.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\giordani versiani\\anaconda3\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain-huggingface) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'Foi forçado o cancelamento de uma conexão existente pelo host remoto', None, 10054, None))': /simple/langchain-faiss-cpu/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'Foi forçado o cancelamento de uma conexão existente pelo host remoto', None, 10054, None))': /simple/langchain-faiss-cpu/\n",
      "ERROR: Could not find a version that satisfies the requirement langchain-faiss-cpu (from versions: none)\n",
      "ERROR: No matching distribution found for langchain-faiss-cpu\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-text-splitters\n",
    "!pip install langchain-huggingface\n",
    "!pip install langchain-faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e04b7528-716b-4258-aa1e-a7913b4d7065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Inicializando a Pipeline do Chatbot ---\n",
      "SUCESSO: Encontrado o modelo em: D:\\ProjetoIntegrador\\Phi-3-mini-4k-instruct-Q4_K_S.gguf\n",
      "Carregando o Gerador (LLM Phi-3)... Isso pode demorar um pouco.\n",
      "Gerador (LLM) carregado com sucesso!\n",
      "\n",
      "Criando o Buscador (RAG)...\n",
      "Carregando textos de 7 arquivos...\n",
      "Textos carregados. Total de 8697 trechos de texto.\n",
      "Textos divididos em 570 'chunks'.\n",
      "Carregando modelo de embedding (sentence-transformer)...\n",
      "Criando o índice vetorial (FAISS)... Isso pode levar um minuto.\n",
      "Buscador (RAG) criado com sucesso (k=2)!\n",
      "\n",
      "Montando a pipeline final...\n",
      "\n",
      " CHATBOT PRONTO PARA USAR! (Versão V2) \n",
      "Rode a próxima célula para fazer perguntas.\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAÇÕES\n",
    "\n",
    "import os  # O 'os' serve para interagir com o sistema operacional (ex: ver se um arquivo existe)\n",
    "import json  # Nossos dados estão em .jsonl, então precisamos do 'json' para conseguir lê-los\n",
    "import re  # (Regex) É o nosso \"faxineiro\" de texto, usado para tirar sujeira e caracteres estranhos\n",
    "from llama_cpp import Llama  # É a biblioteca que carrega o \"Cérebro\" (o LLM Phi-3)\n",
    "\n",
    "# Bibliotecas do Buscador (RAG)\n",
    "# Essas ferramentas são do LangChain e servem para construir nossa \"Biblioteca\" (o RAG)\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter # A \"tesoura\" que corta textos grandes em pedaços menores\n",
    "from langchain_huggingface import HuggingFaceEmbeddings # O \"tradutor\" que transforma os textos em números (vetores)\n",
    "from langchain_community.vectorstores import FAISS # O \"Google\" particular, nosso banco de dados vetorial super-rápido\n",
    "\n",
    "# Ferramentas Padrão (A \"COLA\" do LangChain)\n",
    "# São as peças que vão conectar tudo\n",
    "from langchain_core.prompts import PromptTemplate # O \"roteiro\" que diz ao LLM como ele deve se comportar\n",
    "from langchain_core.runnables import RunnablePassthrough # Uma \"peça\" que deixa a pergunta do usuário passar direto\n",
    "from langchain_core.output_parsers import StrOutputParser  # Garante que a resposta final seja só o texto, e nada mais\n",
    "\n",
    "print(\"--- Inicializando a Pipeline do Chatbot ---\")\n",
    "\n",
    "# FUNÇÃO PARA LER OS ARQUIVOS .JSONL (O \"BIBLIOTECÁRIO\") \n",
    "# Precisamos de uma \"receita\" (função) para ler todos os nossos arquivos de texto.\n",
    "def carregar_textos_dos_jsonl(arquivos_jsonl):\n",
    "    \"\"\"\n",
    "    Esta função abre uma lista de arquivos .jsonl, lê linha por linha,\n",
    "    e puxa o texto de dentro de cada uma.\n",
    "    \"\"\"\n",
    "    textos_completos = []  # Uma \"caixa\" vazia para guardar todos os textos que encontrarmos\n",
    "    print(f\"Carregando textos de {len(arquivos_jsonl)} arquivos...\")\n",
    "    \n",
    "    # Vamos olhar cada arquivo da nossa lista, um por um\n",
    "    for arquivo in arquivos_jsonl:\n",
    "        # Primeiro, checamos se o arquivo realmente existe no lugar certo\n",
    "        if not os.path.exists(arquivo):\n",
    "            print(f\"AVISO: Arquivo {arquivo} não encontrado. Pulando.\")\n",
    "            continue  # Vai para o próximo arquivo da lista\n",
    "        \n",
    "        # O 'try...except' é um \"cinto de segurança\". Se der erro ao ler, o programa não quebra.\n",
    "        try:\n",
    "            # Abrimos o arquivo (usando 'utf-8' para ler acentos e 'ç' corretamente)\n",
    "            with open(arquivo, 'r', encoding='utf-8') as f:\n",
    "                # Como é .jsonl, lemos linha por linha\n",
    "                for linha in f:\n",
    "                    try:\n",
    "                        # Se a linha estiver vazia, pulamos\n",
    "                        if not linha.strip(): continue\n",
    "                        \n",
    "                        # Transformamos o texto da linha (JSON) em um objeto Python (um dicionário)\n",
    "                        data = json.loads(linha)\n",
    "                        \n",
    "                        # Agora, procuramos o texto!\n",
    "                        if isinstance(data, dict):\n",
    "                            # Às vezes o texto está na chave \"text\"\n",
    "                            if \"text\" in data:\n",
    "                                textos_completos.append(data[\"text\"])\n",
    "                            # Às vezes está na chave \"page_content\" (comum no LangChain)\n",
    "                            elif \"page_content\" in data:\n",
    "                                textos_completos.append(data[\"page_content\"])\n",
    "                        # E se o JSON for só o texto puro?\n",
    "                        elif isinstance(data, str):\n",
    "                            textos_completos.append(data)\n",
    "                            \n",
    "                    except json.JSONDecodeError: pass  # Se a linha não for um JSON válido, a gente só ignora\n",
    "                    except Exception as e_linha: print(f\"Aviso: Erro ao processar linha no arquivo {arquivo}: {e_linha}\")\n",
    "                        \n",
    "        except Exception as e_arquivo:\n",
    "            print(f\"Erro GERAL ao ler o arquivo {arquivo}: {e_arquivo}\")\n",
    "            \n",
    "    print(f\"Textos carregados. Total de {len(textos_completos)} trechos de texto.\")\n",
    "    \n",
    "    # Alerta importante! Se não achamos NADA, o chatbot não vai funcionar.\n",
    "    if len(textos_completos) == 0:\n",
    "        print(\"ALERTA CRÍTICO: Nenhum texto foi carregado.\")\n",
    "        \n",
    "    # No final, juntamos todos os pedacinhos de texto em um \"textão\" gigante\n",
    "    return \"\\n\".join(textos_completos)\n",
    "\n",
    "#CARREGAR O \"GERADOR\" (LLM - O \"CÉREBRO\")\n",
    "# Agora vamos carregar o modelo de linguagem que vai \"pensar\" e escrever as respostas.\n",
    "\n",
    "# O nome do arquivo .gguf do modelo.\n",
    "model_name = \"Phi-3-mini-4k-instruct-Q4_K_S.gguf\"\n",
    "# Pegamos o caminho completo (ex: C:/Usuario/Projeto/modelo.gguf)\n",
    "model_path_absolute = os.path.abspath(model_name)\n",
    "\n",
    "# Verificação de segurança: O arquivo do modelo está onde deveria?\n",
    "if not os.path.exists(model_path_absolute):\n",
    "    # Se não encontrar, o programa para aqui.\n",
    "    print(f\"ERRO CRÍTICO: Não encontrou o modelo em {model_path_absolute}\")\n",
    "else:\n",
    "    # Se encontrou, continuamos\n",
    "    print(f\"SUCESSO: Encontrado o modelo em: {model_path_absolute}\")\n",
    "    print(\"Carregando o Gerador (LLM Phi-3)... Isso pode demorar um pouco.\")\n",
    "    \n",
    "    # Esta é a linha que \"chama\" o cérebro para a memória\n",
    "    llm = Llama(\n",
    "        model_path=model_path_absolute,  # Onde está o arquivo\n",
    "        n_gpu_layers=50,  # 50 camadas do modelo na Placa de Vídeo (GPU)!\n",
    "                          # Isso deixa o chatbot MUITO mais rápido. (Se der erro, tente 0 ou -1)\n",
    "        n_ctx=4096,       # A \"memória de curto prazo\" do modelo. O tamanho do \"papel\" que ele tem\n",
    "                          # para ler o contexto e escrever a resposta (em tokens).\n",
    "        verbose=False     # Para ele não ficar \"falando\" (imprimindo logs) o tempo todo.\n",
    "    )\n",
    "    print(\"Gerador (LLM) carregado com sucesso!\")\n",
    "\n",
    "    # \"BUSCADOR\" (A \"BIBLIOTECA\" RAG) \n",
    "    # Agora vamos pegar nossos documentos, \"quebrar\" e \"indexar\" eles\n",
    "    # para que o chatbot possa fazer buscas rápidas.\n",
    "    print(\"\\nCriando o Buscador (RAG)...\")\n",
    "    \n",
    "    lista_de_documentos = [\n",
    "        \"Estágio_output (1).jsonl\",\n",
    "        \"Estatuto - Fevereiro 2025__output (1).jsonl\",\n",
    "        \"PPCBCC2019_output (1).jsonl\",\n",
    "        \"Regimento Interno dos Campi_output (1).jsonl\",\n",
    "        \"REGIMENTO_GERAL_FEVEREIRO_DE_2025_output (1).jsonl\",\n",
    "        \"RegulamentoCursosGraduação_output (1).jsonl\",\n",
    "        \"Ciencia-da-computacao-01-2025_com_sala_MkIII_output (2).jsonl\"\n",
    "    ]\n",
    "    \n",
    "    # Usamos nossa função \"Bibliotecário\" para ler tudo e juntar no \"textão\"\n",
    "    textos_da_faculdade = carregar_textos_dos_jsonl(lista_de_documentos)\n",
    "\n",
    "    # Se o textão estiver vazio, não temos o que fazer.\n",
    "    if not textos_da_faculdade:\n",
    "        print(\"ERRO: Nenhum texto foi carregado. A pipeline não pode ser criada.\")\n",
    "    else:\n",
    "        # \"Tesoura\" (Splitter)\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,  # Cortar os textos em pedaços (chunks) de 1000 caracteres\n",
    "            chunk_overlap=200 # Cada pedaço novo vai ter 200 caracteres do pedaço anterior\n",
    "                              # (Isso evita cortar uma frase/ideia importante no meio)\n",
    "        )\n",
    "        # Cortando o \"textão\" em uma lista de 'chunks'\n",
    "        chunks = text_splitter.split_text(textos_da_faculdade)\n",
    "        \n",
    "        print(f\"Textos divididos em {len(chunks)} 'chunks'.\")\n",
    "        \n",
    "        # \"Tradutor\" (Embeddings)\n",
    "        print(\"Carregando modelo de embedding (sentence-transformer)...\")\n",
    "        embeddings_model = HuggingFaceEmbeddings(\n",
    "            model_name=\"paraphrase-multilingual-MiniLM-L12-v2\", \n",
    "            model_kwargs={'device': 'cpu'}  # O tradutor é leve, pode rodar na CPU\n",
    "        )\n",
    "        \n",
    "        #  \"Índice\" (Vector Store - FAISS)\n",
    "        # Este é o passo mais importante do RAG.\n",
    "        # Ele pega os 'chunks', usa o 'tradutor' para transformar em números (vetores),\n",
    "        # e armazena tudo no FAISS, que é um índice de busca ultra-rápido.\n",
    "        print(\"Criando o índice vetorial (FAISS)... Isso pode levar um minuto.\")\n",
    "        vector_store = FAISS.from_texts(chunks, embeddings_model)\n",
    "        \n",
    "        #  O \"Buscador\" (Retriever)\n",
    "        # Agora criamos o \"Buscador\" oficial, que usa o índice FAISS\n",
    "        \n",
    "        # MUDAMOS DE 'k=3' PARA 'k=2'\n",
    "        # Isso reduz o \"ruído\" (lixo) enviado para o LLM.\n",
    "        # k=2 significa: \"Quando alguém fizer uma pergunta, me traga os 2 chunks\n",
    "        # mais parecidos com ela.\"\n",
    "        retriever = vector_store.as_retriever(search_kwargs={'k': 2})\n",
    "        \n",
    "        print(\"Buscador (RAG) criado com sucesso (k=2)!\")\n",
    "\n",
    "        # --- 5. MONTAR A PIPELINE (A \"LINHA DE MONTAGEM\") ---\n",
    "        # Agora vamos conectar o \"Buscador\" (RAG) com o \"Gerador\" (LLM).\n",
    "        print(\"\\nMontando a pipeline final...\")\n",
    "\n",
    "        #  \"ROTEIRO\"\n",
    "        # Damos ao Phi-3 permissão para filtrar o lixo e encontrar a resposta certa.\n",
    "        # Este roteiro é a \"instrução\" que o LLM recebe ANTES de ver a pergunta.\n",
    "        prompt_template = \"\"\"<|system|>\n",
    "Você é um assistente prestativo do IFNMG.\n",
    "Sua tarefa é responder a pergunta do usuário.\n",
    "Use os trechos de contexto abaixo para encontrar a resposta mais precisa.\n",
    "O contexto pode conter informações misturadas; seu trabalho é encontrar a informação correta\n",
    "e responder de forma direta e concisa.\n",
    "Responda APENAS com base no contexto fornecido.<|end|>\n",
    "<|user|>\n",
    "CONTEXTO:\n",
    "{context}\n",
    "\n",
    "PERGUNTA:\n",
    "{question}<|end|>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "        # {context} e {question} são \"buracos\" que vamos preencher automaticamente.\n",
    "        # <|assistant|> no final já deixa \"no ponto\" para o LLM começar a resposta.\n",
    "        \n",
    "        # Transformamos o texto do roteiro em um objeto \"PromptTemplate\" do LangChain\n",
    "        prompt = PromptTemplate(\n",
    "            template=prompt_template,\n",
    "            input_variables=[\"context\", \"question\"] # Os \"buracos\" que ele precisa preencher\n",
    "        )\n",
    "        \n",
    "        # FUNÇÃO DE LIMPEZA DO CONTEXTO (O \"FAXINEIRO\")\n",
    "        # O RAG pode trazer textos com caracteres estranhos (ex: ícones 0uf000)\n",
    "        # Esta função vai limpar esse lixo antes de mandar para o LLM.\n",
    "        def formatar_e_limpar_contexto(docs: list) -> str:\n",
    "            textos_limpos = []\n",
    "            for doc in docs:\n",
    "                texto_sujo = doc.page_content # Pegamos o texto do chunk\n",
    "                # 1ª Faxina: Remover caracteres estranhos (tipo  UF000)\n",
    "                texto_limpo = re.sub(r'[\\uf000-\\uf0ff]', ' ', texto_sujo)\n",
    "                # 2ª Faxina: Substituir múltiplos espaços/quebras de linha por um espaço só\n",
    "                texto_limpo = re.sub(r'\\s+', ' ', texto_limpo)\n",
    "                textos_limpos.append(texto_limpo.strip())\n",
    "            # Junta os (k=2) chunks limpos em um texto só, separados por uma linha\n",
    "            return \"\\n\".join(textos_limpos)\n",
    "\n",
    "        # FUNÇÃO DE RESPOSTA (O \"CHAMADOR\" DO LLM) \n",
    "        # Fizemos isso porque o `Llama` do `llama_cpp` funciona um pouco\n",
    "        # diferente do `Llama` padrão do LangChain.\n",
    "        def llm_responder(prompt_como_objeto):\n",
    "            # O LangChain vai entregar um \"objeto\" de prompt.\n",
    "            # O `llama_cpp` só quer uma string de texto. Então a gente converte.\n",
    "            prompt_como_string = prompt_como_objeto.to_string()\n",
    "            \n",
    "            # Mandamos o prompt completo (roteiro + contexto + pergunta) para o \"Cérebro\"\n",
    "            output = llm(\n",
    "                prompt_como_string,\n",
    "                max_tokens=1024,      # A resposta pode ter no máximo 1024 tokens\n",
    "                temperature=0.2,    # 0.0 = 100% focado, 1.0 = 100% criativo.\n",
    "                                    # 0.2 é ótimo para \"focar\" na resposta do RAG.\n",
    "                repeat_penalty=1.1  # Uma leve penalidade se ele começar a se repetir.\n",
    "            )\n",
    "            # A resposta do LLM vem dentro de um JSON. Pegamos só o texto.\n",
    "            return output[\"choices\"][0][\"text\"]\n",
    "\n",
    "        # A PIPELINE FINAL (A \"LINHA DE MONTAGEM\")\n",
    "        # ALangChain. O símbolo `|` (pipe)\n",
    "        # significa: \"o resultado do passo anterior vira a entrada do próximo\".\n",
    "        \n",
    "        chain = (\n",
    "            {\n",
    "                # O processo começa aqui. Quando o usuário perguntar algo:\n",
    "                # 1. A pergunta vai para o 'retriever', que acha os 2 chunks.\n",
    "                # 2. Os 2 chunks são \"pipeados\" para a função 'formatar_e_limpar_contexto'.\n",
    "                # O resultado limpo vira a variável \"context\".\n",
    "                \"context\": retriever | formatar_e_limpar_contexto,\n",
    "                \n",
    "                # 3. A pergunta original do usuário passa direto (Passthrough)\n",
    "                # e vira a variável \"question\".\n",
    "                \"question\": RunnablePassthrough()\n",
    "            }\n",
    "            # 4. As variáveis \"context\" e \"question\" são encaixadas no \"roteiro\" (prompt)\n",
    "            | prompt\n",
    "            # 5. O roteiro completo é \"pipeado\" para o \"Cérebro\" (llm_responder)\n",
    "            | llm_responder\n",
    "            # 6. A resposta final do \"Cérebro\" é \"pipeada\" para o \"Limpador de Saída\",\n",
    "            # que garante que teremos só o texto puro.\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        print(\"\\n CHATBOT PRONTO PARA USAR! (Versão V2) \")\n",
    "        print(\"Rode a próxima célula para fazer perguntas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dddbf6e-e41d-438d-8ab7-7729b7d90fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando resposta...\n",
      "\n",
      "--- Resposta ---\n",
      " Os professores que podem ser orientadores do TCC são os professores de Projeto de Trabalho de Conclusão de Curso (PTCC) no IFNMG - Campus Montes Claros.\n"
     ]
    }
   ],
   "source": [
    "print(\"Buscando resposta...\")\n",
    "\n",
    "# --- FAÇA SUA PERGUNTA AQUI ---\n",
    "pergunta = \"Quais são os professores que podem ser orientadores do TCC?\"\n",
    "# ---\n",
    "\n",
    "resposta = chain.invoke(pergunta)\n",
    "\n",
    "print(\"\\n--- Resposta ---\")\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e213f6f-a63c-4ac8-8ba1-1a0089c15098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando resposta...\n",
      "\n",
      "--- Resposta ---\n",
      " O tempo máximo permitido para que um estudante conclua o curso de Ciência da Computação é 3280 horas.\n"
     ]
    }
   ],
   "source": [
    "print(\"Buscando resposta...\")\n",
    "\n",
    "# --- FAÇA SUA PERGUNTA AQUI ---\n",
    "pergunta = \"Qual é o tempo máximo permitido para que um estudante conclua o curso de Ciência da Computação?\"\n",
    "# ---\n",
    "\n",
    "resposta = chain.invoke(pergunta)\n",
    "\n",
    "print(\"\\n--- Resposta ---\")\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20b94661-5613-49e8-8aa2-6db1f246b456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando resposta...\n",
      "\n",
      "--- Resposta ---\n",
      " O aluno sabe se foi aprovado ou reprovado numa matéria observando a obrigatoriedade da realização de, no mínimo, 03 (três) avaliações parciais no período letivo e sendo informado pelo professor sobre os resultados obtidos nos instrumentos de avaliação.\n"
     ]
    }
   ],
   "source": [
    "print(\"Buscando resposta...\")\n",
    "\n",
    "# --- FAÇA SUA PERGUNTA AQUI ---\n",
    "pergunta = \"Como o aluno sabe se foi aprovado ou reprovado numa matéria?\"\n",
    "# ---\n",
    "\n",
    "resposta = chain.invoke(pergunta)\n",
    "\n",
    "print(\"\\n--- Resposta ---\")\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "799d603f-a180-49fa-a71d-0a4e6b4b0ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando resposta...\n",
      "\n",
      "--- Resposta ---\n",
      " As Atividades Complementares no curso de Ciência da Computação totalizam 160 horas.\n",
      "\n",
      "(Note: The context does not specify the types of activities that count as \"Complementary Activities,\" so only the number of hours is provided.)\n"
     ]
    }
   ],
   "source": [
    "print(\"Buscando resposta...\")\n",
    "\n",
    "# --- FAÇA SUA PERGUNTA AQUI ---\n",
    "pergunta = \"Quantas horas e que tipo de atividades contam como Atividades Complementares no curso de Ciência da Computação?\"\n",
    "# ---\n",
    "\n",
    "resposta = chain.invoke(pergunta)\n",
    "\n",
    "print(\"\\n--- Resposta ---\")\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24f86702-6238-4104-b881-0c0e7e1b6e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando resposta...\n",
      "\n",
      "--- Resposta ---\n",
      " O estágio de Ciência da Computação pode começar após o período integral do curso, uma vez que a matrícula é adotada e os estudantes já possuem conhecimento básico necessário.\n"
     ]
    }
   ],
   "source": [
    "print(\"Buscando resposta...\")\n",
    "\n",
    "# --- FAÇA SUA PERGUNTA AQUI ---\n",
    "pergunta = \" Em qual momento do curso a gente pode começar a fazer o estágio de Ciência da Computação?\"\n",
    "# ---\n",
    "\n",
    "resposta = chain.invoke(pergunta)\n",
    "\n",
    "print(\"\\n--- Resposta ---\")\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087894a1-05e1-4305-9be2-000fa1e45fba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
